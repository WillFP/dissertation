python modeling/evaluation/train.py \
  --latent-dim=128 \
  --batch-size=8192 \
  --precompute-latents \
  --epochs=5000 \
  --autoencoder-path=models/autoencoder_250k.pt \
  --path=models/eval_conv_1m.pt \
  --data=datasets/positions_1m.h5 \
  --num-workers=16 \
  --learning-rate=0.004 \
  --min-learning-rate=0.000001 > output.log 2>&1 &
